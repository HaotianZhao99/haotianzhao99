{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "QclF3kdDE8Xf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QclF3kdDE8Xf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "project_path = \"/content/drive/MyDrive/PopBERT\"\n",
    "os.chdir(project_path)\n",
    "\n",
    "import sys\n",
    "sys.path.append(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0db552f",
   "metadata": {
    "id": "e0db552f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import src\n",
    "from src.bert import training\n",
    "from src.bert.dataset import PBertDataset\n",
    "from src.bert.dataset.strategies import MLMin1PopIdeol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a3b191f",
   "metadata": {
    "id": "4a3b191f"
   },
   "outputs": [],
   "source": [
    "EXCLUDE_CODERS: list[str] = []\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MODEL = \"deepset/gbert-large\"   # Pre-trained German BERT model to be used (GBert-large)\n",
    "BATCH_SIZE = 8          # Training batch size\n",
    "N_EPOCHS = 3           # Number of training epochs\n",
    "LR = 0.000009          # Learning rate\n",
    "WEIGHT_DECAY = 0.01       # Weight decay (L2 regularization coefficient)\n",
    "\n",
    "THRESHOLDS = {0: 0.415961, 1: 0.295400, 2: 0.429109, 3: 0.302714}   # Decision thresholds for different categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kz49T0LBF9M1",
   "metadata": {
    "id": "kz49T0LBF9M1"
   },
   "source": [
    "This code defines critical hyperparameters and configurations for model training. It uses a German BERT model with carefully tuned parameters for what appears to be a multi-label classification task.\n",
    "\n",
    "The unique thresholds (all deviating from the default 0.5) reveal a sophisticated optimization approach: high-precision decimals indicate fine-tuned calibration, while varying thresholds across categories reflect class-specific characteristics. Higher thresholds for categories 0 and 2 demand stronger confidence, while lower ones for categories 1 and 3 allow more lenient classification. This precision suggests rigorous testing and optimization tailored to each category's distinct requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ed884b",
   "metadata": {
    "id": "37ed884b"
   },
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "train = PBertDataset.from_disk(\n",
    "    path=src.PATH / \"data/labeled_data/train.csv.zip\",\n",
    "    label_strategy=MLMin1PopIdeol(),\n",
    "    exclude_coders=EXCLUDE_CODERS,\n",
    ")\n",
    "\n",
    "# Load test dataset\n",
    "test = PBertDataset.from_disk(\n",
    "    path=src.PATH / \"data/labeled_data/test.csv.zip\",\n",
    "    label_strategy=MLMin1PopIdeol(),\n",
    "    exclude_coders=EXCLUDE_CODERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece9d3fd",
   "metadata": {
    "id": "ece9d3fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)  # Load tokenizer from pre-trained model\n",
    "collate_fn = train.create_collate_fn(tokenizer)   # Create collate function for batch processing\n",
    "\n",
    "train_loader = DataLoader(train, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test, collate_fn=collate_fn, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c1d390",
   "metadata": {
    "id": "47c1d390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with classifier layers. Training required for optimal performance.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=train.num_labels).to(\n",
    "    DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mk62oXw1H6Qc",
   "metadata": {
    "id": "mk62oXw1H6Qc"
   },
   "source": [
    "This code initializes the classification model using the pre-trained GBert-large model. It automatically configures the final classification layer based on the number of labels in the training set and moves the model to the appropriate computing device."
   ]
  }
],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 }
}