{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7545f8ab",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation and Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54c43f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import stats\n",
    "\n",
    "# Load word vector data\n",
    "tokens_df = pd.read_csv('D:\\Downloads\\info_token.csv\\info_token.csv', names=['token_id', 'word_vector'])\n",
    "\n",
    "# Load answer data with structured column names\n",
    "answer_columns = [\n",
    "    'answer_id', 'question_id', 'anonymous', 'author_id', 'labeled_high_value',\n",
    "    'recommended_by_editor', 'create_timestamp', 'contain_pictures', 'contain_videos',\n",
    "    'thanks_count', 'likes_count', 'comments_count', 'collections_count',\n",
    "    'dislikes_count', 'reports_count', 'helpless_count', 'token_ids', 'topic_ids'\n",
    "]\n",
    "answers_df = pd.read_csv('D:\\Downloads\\info_answer.csv\\info_answer.csv', names=answer_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21880e",
   "metadata": {},
   "source": [
    "We begin by importing essential data processing libraries and loading two primary data files:\n",
    "\n",
    "- info_token.csv: Contains token IDs and their corresponding word vectors, which are pre-trained on large-scale Zhihu content\n",
    "- info_answer.csv: Contains answer features and interaction metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72802bd6",
   "metadata": {},
   "source": [
    "## Step 2: Word Vector Dictionary Construction\n",
    "This step optimizes our data structure for efficient vector operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41cc50c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Building token vectors dictionary...\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: Building token vectors dictionary...\")\n",
    "token_vectors = {}\n",
    "for _, row in tokens_df.iterrows():\n",
    "    token_id = int(row['token_id'])\n",
    "    vector = np.array([float(x) for x in row['word_vector'].split()])\n",
    "    token_vectors[token_id] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f89e15",
   "metadata": {},
   "source": [
    "In this step, we transform the raw word vector data into an optimized dictionary structure. Specifically, we create a dictionary where token IDs serve as keys, mapped to their corresponding 64-dimensional vectors as values.\n",
    "\n",
    "The original data represents vectors as space-separated numeric strings (e.g., \"0.123 0.456 ...\"), a format not conducive to direct numerical computations. By pre-converting these strings into NumPy arrays, we can enhance efficiency in subsequent vector operations (such as cosine distance calculations and mean computations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f92197",
   "metadata": {},
   "source": [
    "## Step 3: Answer Vector Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c1605",
   "metadata": {},
   "source": [
    "This critical step involves transforming each textual response into a standardized vector representation. The process effectively compresses the semantic content of text into a fixed-dimensional numerical sequence, enabling computational analysis of semantic relationships between different answers. This transformation bridges the gap between natural language and mathematical representation, allowing for systematic comparison of semantic content across responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e40b3814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Computing answer vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Computing answer vectors...\")\n",
    "answer_vectors = {}\n",
    "for _, row in answers_df.iterrows():\n",
    "    if pd.isna(row['token_ids']):\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        tokens = [int(t) for t in str(row['token_ids']).split()]\n",
    "        vectors = [token_vectors[t] for t in tokens if t in token_vectors]\n",
    "        if vectors:\n",
    "            answer_vectors[row['answer_id']] = np.mean(vectors, axis=0)\n",
    "    except (ValueError, KeyError):\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107f2af",
   "metadata": {},
   "source": [
    "## Vector Averaging in Semantic Analysis\n",
    "\n",
    "Vector averaging represents a fundamental mathematical operation in analyzing Zhihu answers. For an answer containing n words, where each word is represented as a 64-dimensional vector, the calculation follows the formula:\n",
    "\n",
    "V_answer = (V₁ + V₂ + ... + Vₙ) / n\n",
    "\n",
    "Let's illustrate this with a concrete example. Consider an answer containing the statement: \"AI technology is developing rapidly\"\n",
    "\n",
    "### Word Vector Representations:\n",
    "- V_AI = [0.2, 0.5, ..., 0.3]         # 64-dimensional vector\n",
    "- V_technology = [0.3, 0.4, ..., 0.2]  # 64-dimensional vector\n",
    "- V_developing = [0.1, 0.6, ..., 0.4]  # 64-dimensional vector\n",
    "- V_rapidly = [0.4, 0.3, ..., 0.1]     # 64-dimensional vector\n",
    "\n",
    "### Vector Averaging Computation:\n",
    "V_answer = (V_AI + V_technology + V_developing + V_rapidly) / 4\n",
    "\n",
    "### Mathematical Properties and Semantic Implications\n",
    "\n",
    "#### Geometric Interpretation in Vector Space\n",
    "The vector averaging process carries significant geometric meaning:\n",
    "\n",
    "1. **Point Representation**\n",
    "   - Each word vector represents a unique point in 64-dimensional space\n",
    "   - These points capture specific semantic aspects of individual words\n",
    "\n",
    "2. **Centroid Calculation**\n",
    "   - The final answer vector represents the geometric centroid of all word vectors\n",
    "   - This centroid balances the semantic contributions of all words in the answer\n",
    "\n",
    "3. **Semantic Position**\n",
    "   - The resulting center point represents the overall semantic position of the answer\n",
    "   - This position enables meaningful comparisons between different answers in the semantic space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ef090",
   "metadata": {},
   "source": [
    "##  Step 4: Computing Answer Controversy Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ecd64",
   "metadata": {},
   "source": [
    "Our fundamental approach to analyzing controversy in Zhihu answers rests on a key principle: the controversy level of an answer can be quantified through its semantic divergence from other answers to the same question. This quantification process involves sophisticated vector operations and distance calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e3817",
   "metadata": {},
   "source": [
    "Cosine distance serves as an elegant mathematical tool for measuring semantic differences between answers. It works by calculating the angle between vectors:\n",
    "- Small angles between vectors indicate similar viewpoints\n",
    "- Large angles suggest significant differences in opinion\n",
    "\n",
    "The controversy score for each answer is computed as the mean cosine distance from all other answers within the same question. Higher scores indicate greater divergence from mainstream opinions, suggesting higher controversy.\n",
    "\n",
    "### Practical Example\n",
    "Consider a question: \"Will AI replace human jobs?\"\n",
    "\n",
    "Sample answers with their vector representations:\n",
    "1. Positive view: V1 = [0.2, 0.3, ..., 0.1]\n",
    "2. Negative view: V2 = [0.1, -0.2, ..., 0.3]\n",
    "3. Neutral view: V3 = [0.15, 0.1, ..., 0.2]\n",
    "\n",
    "\n",
    "Calculating controversy score for Answer 1:\n",
    "```python\n",
    "distances = [\n",
    "    cosine(V1, V2),  # ≈ 0.8 (high divergence)\n",
    "    cosine(V1, V3)   # ≈ 0.3 (moderate divergence)\n",
    "]\n",
    "controversy_score_1 = mean(distances) = 0.55\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81951b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Computing answer controversy scores...\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Computing answer controversy scores...\")\n",
    "answer_controversy_scores = {}\n",
    "question_groups = {}\n",
    "\n",
    "# Group answers by question\n",
    "for aid, vector in answer_vectors.items():\n",
    "    qid = answers_df[answers_df['answer_id'] == aid]['question_id'].iloc[0]\n",
    "    if qid not in question_groups:\n",
    "        question_groups[qid] = {}\n",
    "    question_groups[qid][aid] = vector\n",
    "\n",
    "# Calculate controversy scores\n",
    "for qid, answers in question_groups.items():\n",
    "    if len(answers) < 2:  # Skip questions with single answers\n",
    "        continue\n",
    "    \n",
    "    for aid, vector in answers.items():\n",
    "        differences = []\n",
    "        for other_aid, other_vector in answers.items():\n",
    "            if aid != other_aid:\n",
    "                diff = cosine(vector, other_vector)\n",
    "                differences.append(diff)\n",
    "        \n",
    "        if differences:\n",
    "            controversy_score = np.mean(differences)\n",
    "            answer_controversy_scores[aid] = controversy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a733a4e",
   "metadata": {},
   "source": [
    "## Step 5: Computing Question-Level Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0edb68",
   "metadata": {},
   "source": [
    "Beyond individual answer analysis, we need to evaluate controversy at the question level. This requires aggregating answer-level data and calculating comprehensive metrics for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f480398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Computing question-level metrics...\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Computing question-level metrics...\")\n",
    "question_metrics = {}\n",
    "\n",
    "for qid in question_groups.keys():\n",
    "    question_answers = answers_df[answers_df['question_id'] == qid]\n",
    "    \n",
    "  # Calculate total interactions\n",
    "    total_interactions = question_answers.agg({\n",
    "        'likes_count': 'sum',\n",
    "        'dislikes_count': 'sum',\n",
    "        'comments_count': 'sum',\n",
    "        'collections_count': 'sum',\n",
    "        'reports_count': 'sum',\n",
    "        'helpless_count': 'sum',\n",
    "        'thanks_count': 'sum'\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a483a",
   "metadata": {},
   "source": [
    "### Engagement Metrics Categories:\n",
    "\n",
    "#### 1. Positive Engagement\n",
    "- **Likes Count**: Indicates level of approval and agreement\n",
    "- **Thanks Count**: Measures perceived usefulness and value\n",
    "- **Collections Count**: Reflects reference value and content quality\n",
    "\n",
    "#### 2. Negative Engagement\n",
    "- **Dislikes Count**: Measures level of disagreement or disapproval\n",
    "- **Reports Count**: Indicates potential content issues or violations\n",
    "- **Helpless Flags Count**: Reflects low practical utility or unhelpfulness\n",
    "\n",
    "#### 3. Neutral Engagement\n",
    "- **Comments Count**: Measures discussion intensity and engagement level\n",
    "\n",
    "Each metric provides unique insights into how users interact with and evaluate content on the platform, helping us understand different aspects of user response and content impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d46e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating Question-Level Controversy\n",
    "    answer_scores = [answer_controversy_scores.get(aid, 0) \n",
    "                    for aid in question_answers['answer_id']\n",
    "                    if aid in answer_controversy_scores]\n",
    "# Collect controversy scores for all answers under the question\n",
    "# Compute mean as the overall controversy level for the question   \n",
    "\n",
    "\n",
    "    if answer_scores:\n",
    "        question_metrics[qid] = {\n",
    "            'avg_controversy': np.mean(answer_scores),\n",
    "            'total_likes': total_interactions['likes_count'],\n",
    "            'total_dislikes': total_interactions['dislikes_count'],\n",
    "            'total_comments': total_interactions['comments_count'],\n",
    "            'total_collections': total_interactions['collections_count'],\n",
    "            'total_reports': total_interactions['reports_count'],\n",
    "            'total_helpless': total_interactions['helpless_count'],\n",
    "            'total_thanks': total_interactions['thanks_count'],\n",
    "            'total_engagement': sum(total_interactions)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69717dc",
   "metadata": {},
   "source": [
    "## Step 6: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cefdff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Analyzing correlations...\n",
      "\n",
      "Correlation Analysis Results:\n",
      "\n",
      "Pearson Correlations between Controversy and Engagement:\n",
      "likes: 0.0234\n",
      "dislikes: 0.0187\n",
      "comments: 0.0188\n",
      "collections: -0.0162\n",
      "reports: 0.0055\n",
      "helpless: 0.0144\n",
      "thanks: -0.0037\n",
      "total_engagement: 0.0104\n",
      "\n",
      "Spearman Correlations between Controversy and Engagement:\n",
      "likes: 0.1526\n",
      "dislikes: 0.1665\n",
      "comments: 0.1429\n",
      "collections: 0.0938\n",
      "reports: 0.0778\n",
      "helpless: 0.1473\n",
      "thanks: 0.1254\n",
      "total_engagement: 0.1350\n",
      "\n",
      "Basic Statistics:\n",
      "\n",
      "Controversy Score Statistics:\n",
      "count    54758.000000\n",
      "mean         0.225330\n",
      "std          0.162521\n",
      "min          0.000000\n",
      "25%          0.112797\n",
      "50%          0.179288\n",
      "75%          0.289855\n",
      "max          1.464474\n",
      "Name: avg_controversy, dtype: float64\n",
      "\n",
      "Engagement Statistics:\n",
      "count    5.475800e+04\n",
      "mean     8.070194e+03\n",
      "std      3.842108e+04\n",
      "min      0.000000e+00\n",
      "25%      1.700000e+01\n",
      "50%      2.060000e+02\n",
      "75%      2.149750e+03\n",
      "max      1.524389e+06\n",
      "Name: total_engagement, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 6: Analyzing correlations...\")\n",
    "metrics_df = pd.DataFrame.from_dict(question_metrics, orient='index')\n",
    "# This transformation restructures the dictionary data into a tabular format where:\n",
    "# Each row represents a question\n",
    "# Each column represents a metric (controversy or interaction)\n",
    "# We analyzed the relationship between controversy and eight different interaction indicators\n",
    "\n",
    "# Calculate Pearson and Spearman correlation coefficients\n",
    "correlations = {\n",
    "    'Pearson': {\n",
    "        'likes': stats.pearsonr(metrics_df['avg_controversy'], metrics_df['total_likes'])[0],\n",
    "        'dislikes': stats.pearsonr(metrics_df['avg_controversy'], metrics_df['total_dislikes'])[0],\n",
    "        'comments': stats.pearsonr(metrics_df['avg_controversy'], metrics_df['total_comments'])[0],\n",
    "        'collections': stats.pearsonr(metrics_df['avg_controversy'], metrics_df['total_collections'])[0],\n",
    "        'reports': stats.pearsonr(metrics_df['avg_controversy'], metrics_df['total_reports'])[0],\n",
    "        'helpless': stats.pearsonr(metrics_df['avg_controversy'], metrics_df['total_helpless'])[0],\n",
    "        'thanks': stats.pearsonr(metrics_df['avg_controversy'], metrics_df['total_thanks'])[0],\n",
    "        'total_engagement': stats.pearsonr(metrics_df['avg_controversy'], metrics_df['total_engagement'])[0]\n",
    "    },\n",
    "    'Spearman': {\n",
    "        'likes': stats.spearmanr(metrics_df['avg_controversy'], metrics_df['total_likes'])[0],\n",
    "        'dislikes': stats.spearmanr(metrics_df['avg_controversy'], metrics_df['total_dislikes'])[0],\n",
    "        'comments': stats.spearmanr(metrics_df['avg_controversy'], metrics_df['total_comments'])[0],\n",
    "        'collections': stats.spearmanr(metrics_df['avg_controversy'], metrics_df['total_collections'])[0],\n",
    "        'reports': stats.spearmanr(metrics_df['avg_controversy'], metrics_df['total_reports'])[0],\n",
    "        'helpless': stats.spearmanr(metrics_df['avg_controversy'], metrics_df['total_helpless'])[0],\n",
    "        'thanks': stats.spearmanr(metrics_df['avg_controversy'], metrics_df['total_thanks'])[0],\n",
    "        'total_engagement': stats.spearmanr(metrics_df['avg_controversy'], metrics_df['total_engagement'])[0]\n",
    "    }\n",
    "}\n",
    "# Output analysis results\n",
    "print(\"\\nCorrelation Analysis Results:\")\n",
    "print(\"\\nPearson Correlations between Controversy and Engagement:\")\n",
    "for metric, corr in correlations['Pearson'].items():\n",
    "    print(f\"{metric}: {corr:.4f}\")\n",
    "print(\"\\nSpearman Correlations between Controversy and Engagement:\")\n",
    "for metric, corr in correlations['Spearman'].items():\n",
    "    print(f\"{metric}: {corr:.4f}\")\n",
    "# Save results\n",
    "metrics_df.to_csv('controversy_engagement_analysis.csv')\n",
    "pd.DataFrame(correlations).to_csv('controversy_correlations.csv')\n",
    "# Output basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(\"\\nControversy Score Statistics:\")\n",
    "print(metrics_df['avg_controversy'].describe())\n",
    "print(\"\\nEngagement Statistics:\")\n",
    "print(metrics_df['total_engagement'].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
